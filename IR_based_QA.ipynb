{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"testing_data.pickle\", \"rb\") as input_file:\n",
    "    testing_data = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_mat = np.load(\"word_embedding_matrix.npy\").astype(np.float)\n",
    "\n",
    "with open(\"vocabulary.pickle\", \"rb\") as input_file:\n",
    "    voc = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_data.pickle\", \"rb\") as input_file:\n",
    "    training_data = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding(word, voc, e_mat):\n",
    "    if word in voc:\n",
    "        return e_mat[voc[word], :]\n",
    "    else:\n",
    "        return e_mat[0, :]\n",
    "\n",
    "def get_tokenize_sentences(documents):\n",
    "    tokens = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        sents = nltk.sent_tokenize(doc)\n",
    "        for sent in sents:\n",
    "            sent = sent.strip(\".\")\n",
    "            sent = re.sub(r'[,;\":\\']', '', sent)\n",
    "            tokens.extend(nltk.word_tokenize(sent) )\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def get_sent_embedding(sent, voc, emb_mat):\n",
    "    sent_embedding = np.zeros((len(sent), 50))\n",
    "    for i, word in enumerate(sent):\n",
    "        word_embedding = get_word_embedding(word, voc, emb_mat)\n",
    "        sent_embedding[i, :] = word_embedding\n",
    "\n",
    "    sent_embedding = np.mean(sent_embedding, axis=0)\n",
    "    return sent_embedding\n",
    "    \n",
    "def cos_sim(a, b):\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ans = []\n",
    "\n",
    "for t in testing_data[:3]:\n",
    "    ans = dict()\n",
    "    tokenize_sentences = get_tokenize_sentences(t['text'])\n",
    "    tokenize_question = get_tokenize_sentences([t['question']])\n",
    "    q_emb = get_sent_embedding(tokenize_question[0], voc, emb_mat)\n",
    "\n",
    "    sims = np.zeros((len(tokenize_sentences)))\n",
    "    for i, sent in enumerate(tokenize_sentences):\n",
    "        s_emb = get_sent_embedding(sent, voc, emb_mat)\n",
    "        sims[i] = cos_sim(q_emb, s_emb)\n",
    "    \n",
    "    print(sims)\n",
    "    sentences = []\n",
    "    for para in t['text']:\n",
    "        sentences.extend(nltk.sent_tokenize(para))\n",
    "    \n",
    "    ans[\"id\"] = t['id']\n",
    "    ans['question'] = t['question']\n",
    "    ans[\"text\"] = sentences[np.argmax(sims)]\n",
    "    test_ans.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(testing_data[2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse(tree):\n",
    "    \"recursively traverses an nltk.tree.Tree to find named entities\"\n",
    "\n",
    "    items = []\n",
    "\n",
    "    if hasattr(tree, 'label') and tree.label:\n",
    "        if tree.label() == \"NP\":\n",
    "            items.append(' '.join([child[0] for child in tree]))\n",
    "        else:\n",
    "            for child in tree:\n",
    "                items.extend(traverse(child))\n",
    "\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_query(ent_lst, q):\n",
    "    q = nltk.word_tokenize(q)\n",
    "    return True in list(map(lambda x: x in q, ent_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3618/3618 [01:25<00:00, 42.49it/s]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "\n",
    "test_ans = []\n",
    "\n",
    "for t in tqdm(testing_data):\n",
    "    answer = dict()\n",
    "    answer[\"id\"] = t['id']\n",
    "    \n",
    "    query = t[\"question\"].lower()\n",
    "    \n",
    "    doc = nlp(t[\"text\"])\n",
    "    answer['text'] = set()\n",
    "    if check_query([\"who\", \"organization\"], query):\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in {\"ORG\", \"PERSON\", \"NORP\"}:\n",
    "                answer['text'].add(ent.text)\n",
    "    elif check_query([\"when\", \"time\", \"month\", \"day\", \"year\"], query):\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in {\"DATE\", \"TIME\", \"CARDINAL\"}:\n",
    "                answer['text'].add(ent.text)\n",
    "    elif check_query([\"where\", \"place\", \"city\", \"country\"], query):\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in {\"GPE\", \"LOC\", \"FACILITY\", \"ORG\"}:\n",
    "                answer['text'].add(ent.text)\n",
    "    elif check_query([\"how much\", \"how many\"], query):\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in {\"PERCENT\", \"QUANTITY\", \"CARDINAL\", \"MONEY\"}:\n",
    "                answer['text'].add(ent.text)\n",
    "    else:\n",
    "        for ent in doc.ents:\n",
    "            if not ent.label_:\n",
    "                answer['text'].add(ent.text)\n",
    "\n",
    "    if not answer['text']:\n",
    "        for chunk in doc.noun_chunks:\n",
    "            answer['text'].add(ent.text)\n",
    "\n",
    "    # delete the entities which already appear in query\n",
    "    answer[\"text\"] = \" \".join(list(answer[\"text\"] - set(nltk.word_tokenize(query))))\n",
    "\n",
    "    test_ans.append(answer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'If one samples a continental group, what do the clusters become?', 'id': 2205, 'text': ' When one samples continental groups, the clusters become continental; if one had chosen other sampling patterns, the clustering would be different.'}\n"
     ]
    }
   ],
   "source": [
    "print(testing_data[2205])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open('test.csv', 'w') as f:\n",
    "    f.write(\"id,answer\\n\")\n",
    "    for t in test_ans:\n",
    "        ans = re.sub(r'[^\\w\\s]', '', t[\"text\"])\n",
    "        f.write(str(t['id']) + ', ' + ans + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
