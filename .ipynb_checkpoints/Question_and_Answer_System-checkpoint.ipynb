{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question & Answer System\n",
    "by ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import urllib\n",
    "import sys\n",
    "import os\n",
    "import zipfile\n",
    "import tarfile\n",
    "import json \n",
    "import hashlib\n",
    "import re\n",
    "import itertools\n",
    "import gensim\n",
    "import string\n",
    "import math\n",
    "import nltk\n",
    "import random\n",
    "import csv\n",
    "import io\n",
    "import spacy\n",
    "import scipy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from importlib import reload\n",
    "# from spacy.en import English\n",
    "from collections import Counter\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.word2vec.Word2Vec'>\n"
     ]
    }
   ],
   "source": [
    "sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)\n",
    "print(type(model))\n",
    "# print(model.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def toWordList(str):\n",
    "    word_list = []\n",
    "    words = str.split()\n",
    "    for word in words:\n",
    "        wordd = word.strip(',.?\\'\\\")(').lower()\n",
    "        word_list.append(wordd)\n",
    "    return word_list\n",
    "\n",
    "documents_list = {}\n",
    "documents_count = {}\n",
    "word_set_dic ={}\n",
    "with open('documents.json','r') as f:\n",
    "    documents = json.load(f)\n",
    "    for dic in documents:\n",
    "        word_set = set()\n",
    "        para_list = []\n",
    "        document_list = []\n",
    "        for sentence in dic[\"text\"]:\n",
    "            word_count=Counter()\n",
    "            word_list = toWordList(sentence)\n",
    "            for word in word_list:\n",
    "                word_count[word]+=1\n",
    "                word_set.add(word)\n",
    "            para_list.append(word_list)\n",
    "            document_list.append(word_count)\n",
    "        documents_list[dic[\"docid\"]] = para_list\n",
    "        documents_count[dic[\"docid\"]] = document_list\n",
    "        word_set_dic[dic[\"docid\"]]=set(word_set)\n",
    "\n",
    "# for key in documents_count:\n",
    "#     print (documents_count[key])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(documents_count[410][0]['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTF(docid,word_count):\n",
    "    tfVector = []\n",
    "    for word in word_set_dic[docid]:\n",
    "        tfVector.append(word_count[word])\n",
    "    return tfVector\n",
    "\n",
    "def getIDF(question,docid):\n",
    "    idfVector = []\n",
    "    for word in word_set_dic[docid]:\n",
    "        if word in question:\n",
    "            count = 0\n",
    "            for para in documents_list[docid]:\n",
    "                if word in para:\n",
    "                    count +=1\n",
    "            if not count == 0:\n",
    "                idfVector.append( float( len(documents_list[docid]) )/count )\n",
    "            else:\n",
    "                idfVector.append(0)\n",
    "        else:\n",
    "            idfVector.append(0)\n",
    "    return idfVector\n",
    "\n",
    "def getParagraph(question,docid):\n",
    "    tfidf_list=[]\n",
    "    for paraIndex in documents_count[docid]:\n",
    "        tfVector = getTF(docid,paraIndex)\n",
    "        idfVector = getIDF(question,docid)\n",
    "        zip_list = list(zip(tfVector,idfVector))\n",
    "        f = lambda x,y:x*y\n",
    "        prod = sum(list(map(f,tfVector,idfVector)))\n",
    "        cosine = math.sqrt(np.sum(np.square(tfVector)))*math.sqrt(np.sum(np.square(idfVector)))\n",
    "        if not cosine == 0:\n",
    "            tfidf_list.append(float(prod)/cosine)\n",
    "        else:\n",
    "            tfidf_list.append(0)\n",
    "#     print (docid,np.argmax(tfidf_list),tfidf_list)\n",
    "#     print(documents[docid][\"text\"][np.argmax(tfidf_list)])\n",
    "    return documents[docid][\"text\"][np.argmax(tfidf_list)],np.argmax(tfidf_list)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3618/3618\r"
     ]
    }
   ],
   "source": [
    "with open('training.json','r') as f:\n",
    "    training = json.load(f)\n",
    "\n",
    "with open('devel.json','r') as f:\n",
    "    develop = json.load(f)\n",
    "    \n",
    "#     develop_data = []\n",
    "#     for question in develop:\n",
    "#         question_word_list = toWordList(question[\"question\"])\n",
    "#         paragraph,index = getParagraph(question_word_list,question[\"docid\"])\n",
    "#         develop_data.append((question[\"question\"],paragraph,index,question[\"answer_paragraph\"],question[\"text\"]))\n",
    "    \n",
    "with open('testing.json','r') as f:\n",
    "    testing = json.load(f)\n",
    "#     answerParagraph = []\n",
    "    test_data = []\n",
    "    t = str(len(testing))\n",
    "    num = 0\n",
    "    for question in testing:\n",
    "        sys.stdout.write(('{0}/'+t+'\\r').format(num + 1))\n",
    "        sys.stdout.flush()\n",
    "        question_word_list = toWordList(question[\"question\"])\n",
    "        paragraph,index = getParagraph(question_word_list,question[\"docid\"])\n",
    "        test_data.append((question[\"question\"],paragraph,index))\n",
    "        num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-e8c0c450bde9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[0mtemp_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[0mtemp_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m             \u001b[0mtesting_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "testing_data = []\n",
    "\n",
    "for doc in documents[410:]:\n",
    "    \n",
    "    tfidf = TfidfVectorizer(tokenizer = nltk.word_tokenize, lowercase=True)\n",
    "    \n",
    "    raw_text = []\n",
    "    for para in doc['text']:\n",
    "        raw_text.extend(nltk.sent_tokenize(para))\n",
    "        \n",
    "    td_mat = tfidf.fit_transform(raw_documents=raw_text)\n",
    "    \n",
    "    for question in testing:\n",
    "        if question['docid'] == doc['docid']:\n",
    "            temp_dict = dict()\n",
    "            \n",
    "            query = tfidf.transform([question['question']])\n",
    "            doc_rank = np.dot(query, td_mat.T).toarray()\n",
    "            \n",
    "            idx=(-doc_rank).argsort()[0][:-1]\n",
    "            \n",
    "            temp_dict[\"question\"],temp_dict[\"id\"] = question['question'],question[\"id\"]\n",
    "            temp_dict[\"text\"] = \"\"\n",
    "            \n",
    "            for i in idx:\n",
    "                temp_dict[\"text\"] += \" \" +raw_text[i]\n",
    "                \n",
    "            temp_dict[\"text\"].append()\n",
    "            testing_data.append(temp_dict)\n",
    "            \n",
    "print(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31\r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TruncatedSVD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-e76a6e8bb460>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;31m#         print (idx)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m \u001b[0mgetParagraph2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m410\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m440\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-37-e76a6e8bb460>\u001b[0m in \u001b[0;36mgetParagraph2\u001b[1;34m(questions, documents, fromid, toid)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mquestion\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mquestions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;31m#         print(question[\"docid\"])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0msvd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0mquestion_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetQuestionVector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"question\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mdocument_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocument_matrix_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"docid\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TruncatedSVD' is not defined"
     ]
    }
   ],
   "source": [
    "with open('testing.json','r') as f:\n",
    "    testing = json.load(f)\n",
    "with open('documents.json','r') as f:\n",
    "    documents = json.load(f)\n",
    "def getQuestionVector(question,tfidf):\n",
    "#     print(type(question))\n",
    "    print(type(tfidf.transform(word_tokenize(question))))\n",
    "    return tfidf.transform(word_tokenize(question))\n",
    "\n",
    "def getDocumentMatrix(document,tfidf):\n",
    "    document_matrix = []\n",
    "    for para in document:\n",
    "        para_matrix = []\n",
    "        for sentence in nltk.sent_tokenize(para):\n",
    "            sentence_matrix = tfidf.fit_transform(word_tokenize(sentence))\n",
    "            para_matrix.append(sentence_matrix)\n",
    "        document_matrix.append(para_matrix)\n",
    "    return document_matrix\n",
    "\n",
    "def getDocumentMatrixDict(documents,fromid,toid,tfidf):\n",
    "    \n",
    "#     t = toid-fromid+1\n",
    "    \n",
    "    \n",
    "    document_matrix_dict = {}\n",
    "    for i in range (fromid,toid+1):\n",
    "        sys.stdout.write(('{0}/'+str(toid-fromid+1)+'\\r').format(i-fromid + 1))\n",
    "        sys.stdout.flush()\n",
    "        id,text = documents[i][\"docid\"],documents[i][\"text\"]\n",
    "        if id == i:\n",
    "            document_matrix_dict[id] = getDocumentMatrix(text,tfidf)\n",
    "        else:\n",
    "            print(\"Index Error in getDocumentMatrixDict\")\n",
    "    return document_matrix_dict\n",
    "\n",
    "# tfidf = TfidfVectorizer(tokenizer=nltk.word_tokenize, lowercase=True)\n",
    "# documents_matrix_dict = getDocumentMatrixDict(documents,410,440,tfidf)\n",
    "# print(documents_matrix_dict)\n",
    "\n",
    "def getParagraph2(questions, documents, fromid, toid):\n",
    "    tfidf = TfidfVectorizer(tokenizer=nltk.word_tokenize, lowercase=True)\n",
    "    document_matrix_dict = getDocumentMatrixDict(documents,fromid,toid,tfidf)\n",
    "#     for key in document_matrix_dict:\n",
    "#         print(key,document_matrix_dict[key])\n",
    "    for question in questions:\n",
    "#         print(question[\"docid\"])\n",
    "        svd = TruncatedSVD(n_components=8)\n",
    "        question_vector = getQuestionVector(question[\"question\"],tfidf)\n",
    "        document_matrix = document_matrix_dict[question[\"docid\"]]\n",
    "#         print(document_matrix)\n",
    "        for para_matrix in document_matrix:\n",
    "#             print(question)\n",
    "            for vector in para_matrix:\n",
    "                print(svd.fit_transform(question_vector).shape,sve.fit_transform(vector).shape)\n",
    "#                 print(1-(scipy.spatial.distance.cosine(question_vector,vector)))\n",
    "#         doc_rank = np.dot(question_vector,document_matrix_dict[question[\"docid\"]]).toarray()\n",
    "#         idx = (-doc_rank).argsort()[0][:1]\n",
    "#         print (doc_rank)\n",
    "#         print (idx)\n",
    "        \n",
    "getParagraph2(testing,documents,410,440)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tfidfAcurate(data_list):\n",
    "#     data_count = 0\n",
    "#     correct_count = 0\n",
    "#     for data in data_list:\n",
    "#         data_count+=1\n",
    "# #         print(data[2],data[3])\n",
    "#         if data[2]==data[3]:\n",
    "#             correct_count +=1\n",
    "#     return float(correct_count)/data_count\n",
    "\n",
    "# print(tfidfAcurate(develop_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## nltk chunk\n",
    "def addEntity(sentence):\n",
    "    tagged = pos_tag(word_tokenize(sentence))\n",
    "    entity = nltk.chunk.ne_chunk(tagged)\n",
    "    return entity\n",
    "# def getEntity(data_list):\n",
    "#     for data in data_list:\n",
    "#         question_entity = addEntity(data[0])\n",
    "#         paragraph_entity = addEntity(data[1])\n",
    "\n",
    "def getEntity(sentence):\n",
    "    ## spacy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(sentence)\n",
    "#     for ent in doc.ents:\n",
    "#         print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "    return doc.ents\n",
    "\n",
    "# entity = getEntity(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "# for ent in entity:\n",
    "#     print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def getTextByLabel(entity,label_list):\n",
    "    answer_list = []\n",
    "    for ent in entity:\n",
    "        label = ent.label_\n",
    "        if label in label_list:\n",
    "            answer_list.append(ent.text)\n",
    "    return answer_list\n",
    "\n",
    "def getAnswer(data_list):\n",
    "    t = str(len(data_list))\n",
    "    now = 0\n",
    "    answer=[]\n",
    "    id_num = 0\n",
    "    count = [0,0,0,0,0,0,0]\n",
    "    useList = [0,0]\n",
    "    for data in data_list:\n",
    "        sys.stdout.write(('{0}/'+t+'\\r').format(id_num + 1))\n",
    "        sys.stdout.flush()\n",
    "#         print(count)\n",
    "#         question_entity = getEntity(data[0])\n",
    "        paragraph_entity = getEntity(data[1])\n",
    "        answer_list = list()\n",
    "        \n",
    "        if bool(re.search('what',data[0],re.IGNORECASE)):\n",
    "            count[0]+=1\n",
    "            if bool(re.search('what time',data[0],re.IGNORECASE)):\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"DATE\",\"TIME\"])\n",
    "            else:\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"FACILITY\",\"PRODUCT\",\"WORK_OF_ART\",\"EVENT\",\"ORG\"])\n",
    "        elif bool(re.search('how',data[0],re.IGNORECASE)):\n",
    "            count[1]+=1\n",
    "            if bool(re.search('how much',data[0],re.IGNORECASE) or re.search('how many',data[0],re.IGNORECASE)):\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"MONEY\",\"QUANTITY\",\"CARDINAL\"])\n",
    "            else:\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"LAW\",\"MONEY\",\"QUANTITY\",\"CARDINAL\"])\n",
    "        elif bool(re.search('when',data[0],re.IGNORECASE)):\n",
    "            count[2]+=1\n",
    "            answer_list = getTextByLabel(paragraph_entity,[\"DATE\",\"TIME\"])\n",
    "        elif bool(re.search('who',data[0],re.IGNORECASE)):\n",
    "            count[3]+=1\n",
    "            answer_list = getTextByLabel(paragraph_entity,[\"PERSON\",\"NORP\",\"ORG\"])\n",
    "        elif bool(re.search('where',data[0],re.IGNORECASE)):\n",
    "            count[4]+=1\n",
    "            answer_list = getTextByLabel(paragraph_entity,[\"FACILITY\",\"GPE\",\"LOC\"])\n",
    "        elif bool(re.search('which',data[0],re.IGNORECASE)):\n",
    "            if bool(re.search('which person',data[0],re.IGNORECASE)):\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"PERSON\"])\n",
    "            elif bool(re.search('which time',data[0],re.IGNORECASE)):\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"DATE\",\"TIME\"])\n",
    "            elif bool(re.search('which country',data[0],re.IGNORECASE)):\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"LOC\"])\n",
    "            else:\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"FACILITY\",\"PRODUCT\",\"WORK_OF_ART\",\"EVENT\",\"ORG\"])\n",
    "            count[5]+=1\n",
    "\n",
    "        else:\n",
    "            count[6]+=1\n",
    "        if len(answer_list)==0:\n",
    "            useList[1]+=1\n",
    "            try:\n",
    "                index = random.randint(0,len(paragraph_entity)-1) \n",
    "                answer.append (([str(id_num)+','+paragraph_entity[index].text.replace(\",\", \"\")],data[2]))\n",
    "            except:\n",
    "                index = random.randint(0,len(data[1])-1)\n",
    "                answer.append(([str(id_num)+','+data[1][index].replace(\",\", \"\")],data[2]))\n",
    "        else:\n",
    "            useList[0]+=1\n",
    "            index = random.randint(0,len(answer_list)-1)\n",
    "            answer.append (([str(id_num)+','+answer_list[index].replace(\",\", \"\")],data[2]))\n",
    "        id_num +=1\n",
    "    print(\"count \",count)\n",
    "    print(\"use list \",useList)\n",
    "    return answer\n",
    "\n",
    "    \n",
    "\n",
    "# for line in answer:\n",
    "#     print(line)\n",
    "# count  [2198, 279, 422, 156, 563]\n",
    "# use list  [2757, 861]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count 618 [2198, 333, 268, 418, 153, 197, 51]\n",
      "use list  [2876, 742]\n"
     ]
    }
   ],
   "source": [
    "# answer = getAnswer(test_data)\n",
    "answer = getAnswer(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def campare(question,answer):\n",
    "# #     print(question,answer)\n",
    "# #     print(str(answer[0]).strip(\"'\").split(',')[1][:-2])\n",
    "# #     print(question[4])\n",
    "    \n",
    "#     if question[4]==str(answer[0]).strip(\"'\").split(',')[1][:-2]:\n",
    "#         return True\n",
    "#     else:\n",
    "#         return False\n",
    "\n",
    "# def answerAcurate(data_list,answer):\n",
    "#     data_count = 0\n",
    "#     correct_count = 0\n",
    "#     results = map(campare,data_list,answer)\n",
    "#     for result in results:\n",
    "#         data_count+=1\n",
    "#         if result:\n",
    "#             correct_count +=1\n",
    "#     return float(correct_count)/data_count\n",
    "\n",
    "# print(answerAcurate(develop_data,answer))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line 81 cannot write： ['81,Galschiøt']\n",
      "line 136 cannot write： ['136,Galschiøt']\n",
      "line 220 cannot write： ['220,Galschiøt']\n",
      "line 222 cannot write： ['222,Galschiøt']\n",
      "line 261 cannot write： ['261,Điếu Cày']\n",
      "line 558 cannot write： ['558,£100m']\n",
      "line 806 cannot write： ['806,Aškuzai/Iškuzai']\n",
      "line 835 cannot write： ['835,Ποσειδώνας']\n",
      "line 1075 cannot write： ['1075,Calçada Portuguesa']\n",
      "line 1095 cannot write： ['1095,Calçada Portuguesa']\n",
      "line 1096 cannot write： ['1096,the River Mureş']\n",
      "line 1163 cannot write： ['1163,Calçada Portuguesa']\n",
      "line 1166 cannot write： ['1166,Calçada Portuguesa']\n",
      "line 1353 cannot write： ['1353,Åland Islands']\n",
      "line 1928 cannot write： ['1928,Mötley Crüe']\n",
      "line 2188 cannot write： [\"2188,François Bernier's\"]\n",
      "line 2259 cannot write： ['2259,Štrkalj']\n",
      "line 2328 cannot write： [\"2328,François Bernier's\"]\n",
      "line 2388 cannot write： ['2388,the Norte Chico Civilization Formative Mesoamerica and Ancient Hawaiʻi']\n",
      "line 2392 cannot write： ['2392,the Norte Chico Civilization Formative Mesoamerica and Ancient Hawaiʻi']\n",
      "line 2405 cannot write： ['2405,the Norte Chico Civilization Formative Mesoamerica and Ancient Hawaiʻi']\n",
      "line 2410 cannot write： ['2410,Göbekli Tepe']\n",
      "line 2671 cannot write： ['2671,between £4.99 and £11.99']\n",
      "line 2740 cannot write： ['2740,El Niño']\n",
      "line 3465 cannot write： ['3465,אַ שפּראַך איז אַ דיאַלעקט מיט אַן אַרמײ און פֿלאָט\"\\u200e:']\n"
     ]
    }
   ],
   "source": [
    "##Write into csv file\n",
    "def writeFile(answer):\n",
    "#     sys.stdout = io.TextIOWrapper(sys.stdout.buffer,encoding='utf8')\n",
    "    out = open('answer.csv','a',newline='')\n",
    "    csv_write =csv.writer(out,dialect='excel')\n",
    "    csv_write.writerow([\"id,answer\"])\n",
    "    answerID = 0\n",
    "    for line in answer:\n",
    "        try:\n",
    "            csv_write.writerow(line[0])\n",
    "        except:\n",
    "            print(\"line \"+str(answerID)+\" cannot write： \"+str(line[0]))\n",
    "        answerID+=1\n",
    "    out.close()\n",
    "    \n",
    "writeFile(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
