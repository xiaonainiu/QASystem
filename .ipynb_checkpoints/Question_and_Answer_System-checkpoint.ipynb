{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question & Answer System\n",
    "by ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import urllib\n",
    "import sys\n",
    "import os\n",
    "import zipfile\n",
    "import tarfile\n",
    "import json \n",
    "import hashlib\n",
    "import re\n",
    "import itertools\n",
    "import gensim\n",
    "import string\n",
    "import math\n",
    "import nltk\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.word2vec.Word2Vec'>\n"
     ]
    }
   ],
   "source": [
    "sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)\n",
    "print(type(model))\n",
    "# print(model.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def toWordList(str):\n",
    "    word_list = []\n",
    "    words = str.split()\n",
    "    for word in words:\n",
    "        wordd = word.strip(',.?\\'\\\")(').lower()\n",
    "        word_list.append(wordd)\n",
    "    return word_list\n",
    "\n",
    "documents_list = {}\n",
    "documents_count = {}\n",
    "word_set_dic ={}\n",
    "with open('documents.json','r') as f:\n",
    "    documents = json.load(f)\n",
    "    for dic in documents:\n",
    "        word_set = set()\n",
    "        para_list = []\n",
    "        document_list = []\n",
    "        for sentence in dic[\"text\"]:\n",
    "            word_count=Counter()\n",
    "            word_list = toWordList(sentence)\n",
    "            for word in word_list:\n",
    "                word_count[word]+=1\n",
    "                word_set.add(word)\n",
    "            para_list.append(word_list)\n",
    "            document_list.append(word_count)\n",
    "        documents_list[dic[\"docid\"]] = para_list\n",
    "        documents_count[dic[\"docid\"]] = document_list\n",
    "        word_set_dic[dic[\"docid\"]]=set(word_set)\n",
    "\n",
    "# for key in documents_count:\n",
    "#     print (documents_count[key])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(documents_count[410][0]['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTF(docid,word_count):\n",
    "    tfVector = []\n",
    "    for word in word_set_dic[docid]:\n",
    "        tfVector.append(word_count[word])\n",
    "    return tfVector\n",
    "\n",
    "def getIDF(question,docid):\n",
    "    idfVector = []\n",
    "    for word in word_set_dic[docid]:\n",
    "        if word in question:\n",
    "            count = 0\n",
    "            for para in documents_list[docid]:\n",
    "                if word in para:\n",
    "                    count +=1\n",
    "            if not count == 0:\n",
    "                idfVector.append( float( len(documents_list[docid]) )/count )\n",
    "            else:\n",
    "                idfVector.append(0)\n",
    "        else:\n",
    "            idfVector.append(0)\n",
    "    return idfVector\n",
    "\n",
    "def getParagraph(question,docid):\n",
    "    tfidf_list=[]\n",
    "    for paraIndex in documents_count[docid]:\n",
    "        tfVector = getTF(docid,paraIndex)\n",
    "        idfVector = getIDF(question,docid)\n",
    "        zip_list = list(zip(tfVector,idfVector))\n",
    "#         print(zip_list)\n",
    "        f = lambda x,y:x*y\n",
    "#         print(f(2,3))\n",
    "        prod = sum(list(map(f,tfVector,idfVector)))\n",
    "#         print(sum(list(map(f,tfVector,idfVector))))\n",
    "#         print(math.sqrt(np.sum(np.square(tfVector))))\n",
    "#         print('----------------------------------------------')\n",
    "#         print(len(tfVector))\n",
    "#         print(len(idfVector))\n",
    "#         print(tfVector)\n",
    "#         print('----------------------------------------------')\n",
    "#         print(idfVector)\n",
    "#         print(\"==============================================\")\n",
    "        cosine = math.sqrt(np.sum(np.square(tfVector)))*math.sqrt(np.sum(np.square(idfVector)))\n",
    "        if not cosine == 0:\n",
    "            tfidf_list.append(float(prod)/cosine)\n",
    "        else:\n",
    "            tfidf_list.append(0)\n",
    "#     print (np.argmax(tfidf_list),tfidf_list)\n",
    "    return np.argmax(tfidf_list)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('training.json','r') as f:\n",
    "    training = json.load(f)\n",
    "\n",
    "with open('devel.json','r') as f:\n",
    "    develop = json.load(f)\n",
    "    \n",
    "with open('testing.json','r') as f:\n",
    "    testing = json.load(f)\n",
    "    answer = []\n",
    "    for question in testing:\n",
    "        question_word_list = toWordList(question[\"question\"])\n",
    "        index = getParagraph(question_word_list,question[\"docid\"])\n",
    "#         print(documents[question[\"docid\"]][\"text\"][index])\n",
    "        answer.append(documents[question[\"docid\"]][\"text\"][index])\n",
    "# print(len(training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NNP'), ('my', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Derek', 'NNP'), ('.', '.'), ('I', 'PRP'), ('live', 'VBP'), ('in', 'IN'), ('Salt', 'NNP'), ('Lake', 'NNP'), ('city', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "text = \"Hello my name is Derek. I live in Salt Lake city.\"\n",
    "tagged = pos_tag(word_tokenize(text))\n",
    "print(tagged)\n",
    "entity = nltk.chunk.ne_chunk(tagged)\n",
    "print (entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Hello/NNP\n",
      "  my/PRP$\n",
      "  name/NN\n",
      "  is/VBZ\n",
      "  (PERSON Derek/NNP)\n",
      "  ./.\n",
      "  I/PRP\n",
      "  live/VBP\n",
      "  in/IN\n",
      "  (GPE Salt/NNP Lake/NNP)\n",
      "  city/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Write into csv file\n",
    "out = open('answer.csv','a',newline='')\n",
    "csv_write =csv.writer(out,dialect='csv')\n",
    "csv_write.writerow(\"id,answer\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
