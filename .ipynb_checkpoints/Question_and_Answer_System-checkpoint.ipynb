{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question & Answer System\n",
    "by ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ES\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "C:\\Users\\ES\\Anaconda3\\lib\\site-packages\\gensim-3.4.0-py3.6-win-amd64.egg\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import urllib\n",
    "import sys\n",
    "import os\n",
    "import zipfile\n",
    "import tarfile\n",
    "import json \n",
    "import hashlib\n",
    "import re\n",
    "import itertools\n",
    "import gensim\n",
    "import string\n",
    "import math\n",
    "import nltk\n",
    "import random\n",
    "import csv\n",
    "import io\n",
    "import spacy\n",
    "# from spacy.en import English\n",
    "from collections import Counter\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.tree import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.word2vec.Word2Vec'>\n"
     ]
    }
   ],
   "source": [
    "sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)\n",
    "print(type(model))\n",
    "# print(model.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def toWordList(str):\n",
    "    word_list = []\n",
    "    words = str.split()\n",
    "    for word in words:\n",
    "        wordd = word.strip(',.?\\'\\\")(').lower()\n",
    "        word_list.append(wordd)\n",
    "    return word_list\n",
    "\n",
    "documents_list = {}\n",
    "documents_count = {}\n",
    "word_set_dic ={}\n",
    "with open('documents.json','r') as f:\n",
    "    documents = json.load(f)\n",
    "    for dic in documents:\n",
    "        word_set = set()\n",
    "        para_list = []\n",
    "        document_list = []\n",
    "        for sentence in dic[\"text\"]:\n",
    "            word_count=Counter()\n",
    "            word_list = toWordList(sentence)\n",
    "            for word in word_list:\n",
    "                word_count[word]+=1\n",
    "                word_set.add(word)\n",
    "            para_list.append(word_list)\n",
    "            document_list.append(word_count)\n",
    "        documents_list[dic[\"docid\"]] = para_list\n",
    "        documents_count[dic[\"docid\"]] = document_list\n",
    "        word_set_dic[dic[\"docid\"]]=set(word_set)\n",
    "\n",
    "# for key in documents_count:\n",
    "#     print (documents_count[key])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(documents_count[410][0]['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTF(docid,word_count):\n",
    "    tfVector = []\n",
    "    for word in word_set_dic[docid]:\n",
    "        tfVector.append(word_count[word])\n",
    "    return tfVector\n",
    "\n",
    "def getIDF(question,docid):\n",
    "    idfVector = []\n",
    "    for word in word_set_dic[docid]:\n",
    "        if word in question:\n",
    "            count = 0\n",
    "            for para in documents_list[docid]:\n",
    "                if word in para:\n",
    "                    count +=1\n",
    "            if not count == 0:\n",
    "                idfVector.append( float( len(documents_list[docid]) )/count )\n",
    "            else:\n",
    "                idfVector.append(0)\n",
    "        else:\n",
    "            idfVector.append(0)\n",
    "    return idfVector\n",
    "\n",
    "def getParagraph(question,docid):\n",
    "    tfidf_list=[]\n",
    "    for paraIndex in documents_count[docid]:\n",
    "        tfVector = getTF(docid,paraIndex)\n",
    "        idfVector = getIDF(question,docid)\n",
    "        zip_list = list(zip(tfVector,idfVector))\n",
    "        f = lambda x,y:x*y\n",
    "        prod = sum(list(map(f,tfVector,idfVector)))\n",
    "        cosine = math.sqrt(np.sum(np.square(tfVector)))*math.sqrt(np.sum(np.square(idfVector)))\n",
    "        if not cosine == 0:\n",
    "            tfidf_list.append(float(prod)/cosine)\n",
    "        else:\n",
    "            tfidf_list.append(0)\n",
    "#     print (docid,np.argmax(tfidf_list),tfidf_list)\n",
    "#     print(documents[docid][\"text\"][np.argmax(tfidf_list)])\n",
    "    return documents[docid][\"text\"][np.argmax(tfidf_list)],np.argmax(tfidf_list)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('training.json','r') as f:\n",
    "    training = json.load(f)\n",
    "\n",
    "with open('devel.json','r') as f:\n",
    "    develop = json.load(f)\n",
    "    \n",
    "    develop_data = []\n",
    "    for question in develop:\n",
    "        question_word_list = toWordList(question[\"question\"])\n",
    "        paragraph,index = getParagraph(question_word_list,question[\"docid\"])\n",
    "        develop_data.append((question[\"question\"],paragraph,index,question[\"answer_paragraph\"],question[\"text\"]))\n",
    "    \n",
    "# with open('testing.json','r') as f:\n",
    "#     testing = json.load(f)\n",
    "#     answerParagraph = []\n",
    "#     test_data = []\n",
    "#     for question in testing:\n",
    "#         question_word_list = toWordList(question[\"question\"])\n",
    "#         paragraph,index = getParagraph(question_word_list,question[\"docid\"])\n",
    "#         test_data.append((question[\"question\"],paragraph,index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6951888924765902\n"
     ]
    }
   ],
   "source": [
    "def tfidfAcurate(data_list):\n",
    "    data_count = 0\n",
    "    correct_count = 0\n",
    "    for data in data_list:\n",
    "        data_count+=1\n",
    "#         print(data[2],data[3])\n",
    "        if data[2]==data[3]:\n",
    "            correct_count +=1\n",
    "    return float(correct_count)/data_count\n",
    "\n",
    "print(tfidfAcurate(develop_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## nltk chunk\n",
    "def addEntity(sentence):\n",
    "    tagged = pos_tag(word_tokenize(sentence))\n",
    "    entity = nltk.chunk.ne_chunk(tagged)\n",
    "    return entity\n",
    "# def getEntity(data_list):\n",
    "#     for data in data_list:\n",
    "#         question_entity = addEntity(data[0])\n",
    "#         paragraph_entity = addEntity(data[1])\n",
    "\n",
    "def getEntity(sentence):\n",
    "    ## spacy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(sentence)\n",
    "#     for ent in doc.ents:\n",
    "#         print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "    return doc.ents\n",
    "\n",
    "# entity = getEntity(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "# for ent in entity:\n",
    "#     print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def getTextByLabel(entity,label_list):\n",
    "    answer_list = []\n",
    "    for ent in entity:\n",
    "        label = ent.label_\n",
    "        if label in label_list:\n",
    "            answer_list.append(ent.text)\n",
    "    return answer_list\n",
    "\n",
    "def getAnswer(data_list):\n",
    "    t = len(data_list)\n",
    "    now = 0\n",
    "    answer=[]\n",
    "    id_num = 0\n",
    "    count = [0,0,0,0,0]\n",
    "    useList = [0,0]\n",
    "    for data in data_list:\n",
    "        sys.stdout.write(('{0}/'+t+'\\r').format(id_num + 1))\n",
    "        sys.stdout.flush()\n",
    "#         print(count)\n",
    "        question_entity = getEntity(data[0])\n",
    "        paragraph_entity = getEntity(data[1])\n",
    "        answer_list = list()\n",
    "        \n",
    "        if bool(re.search('what',data[0],re.IGNORECASE)):\n",
    "            count[0]+=1\n",
    "            answer_list = getTextByLabel(paragraph_entity,[\"FACILITY\",\"PRODUCT\",\"WORK_OF_ART\",\"EVENT\",\"ORG\"])\n",
    "        elif bool(re.search('when',data[0],re.IGNORECASE)):\n",
    "            count[1]+=1\n",
    "            answer_list = getTextByLabel(paragraph_entity,[\"DATE\",\"TIME\"])\n",
    "        elif bool(re.search('who',data[0],re.IGNORECASE)):\n",
    "            count[2]+=1\n",
    "            answer_list = getTextByLabel(paragraph_entity,[\"PERSON\",\"NORP\",\"ORG\"])\n",
    "        elif bool(re.search('where',data[0],re.IGNORECASE)):\n",
    "            count[3]+=1\n",
    "            answer_list = getTextByLabel(paragraph_entity,[\"FACILITY\",\"GPE\",\"LOC\"])\n",
    "        else:\n",
    "            count[4]+=1\n",
    "        if len(answer_list)==0:\n",
    "            useList[1]+=1\n",
    "            try:\n",
    "                index = random.randint(0,len(paragraph_entity)-1) \n",
    "                answer.append (([str(id_num)+','+paragraph_entity[index].text.replace(\",\", \"\")],data[2]))\n",
    "            except:\n",
    "                index = random.randint(0,len(data[1]))\n",
    "                answer.append(([str(id_num)+','+data[1][index].replace(\",\", \"\")],data[2]))\n",
    "        else:\n",
    "            useList[0]+=1\n",
    "            index = random.randint(0,len(answer_list)-1)\n",
    "            answer.append (([str(id_num)+','+answer_list[index].replace(\",\", \"\")],data[2]))\n",
    "        id_num +=1\n",
    "    print(\"count \",count)\n",
    "    print(\"use list \",useList)\n",
    "    return answer\n",
    "\n",
    "    \n",
    "\n",
    "# for line in answer:\n",
    "#     print(line)\n",
    "# count  [2198, 279, 422, 156, 563]\n",
    "# use list  [2757, 861]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer = getAnswer(test_data)\n",
    "answer = getAnswer(develop_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def campare(question,answer):\n",
    "#     print(question,answer)\n",
    "#     print(str(answer[0]).strip(\"'\").split(',')[1][:-2])\n",
    "#     print(question[4])\n",
    "    \n",
    "    if question[4]==str(answer[0]).strip(\"'\").split(',')[1][:-2]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def answerAcurate(data_list,answer):\n",
    "    data_count = 0\n",
    "    correct_count = 0\n",
    "    results = map(campare,data_list,answer)\n",
    "    for result in results:\n",
    "        data_count+=1\n",
    "        if result:\n",
    "            correct_count +=1\n",
    "    return float(correct_count)/data_count\n",
    "\n",
    "print(answerAcurate(develop_data,answer))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Write into csv file\n",
    "def writeFile(answer):\n",
    "    # sys.stdout = io.TextIOWrapper(sys.stdout.buffer,encoding='utf8')\n",
    "    out = open('answer.csv','a',newline='')\n",
    "    csv_write =csv.writer(out,dialect='excel')\n",
    "    csv_write.writerow([\"id,answer\"])\n",
    "    answerID = 0\n",
    "    for line in answer:\n",
    "#         print(line[0])\n",
    "        try:\n",
    "            csv_write.writerow(line[0])\n",
    "        except:\n",
    "            print(\"line \"+str(answerID)+\" cannot write\")\n",
    "        answerID+=1\n",
    "    out.close()\n",
    "    \n",
    "writeFile(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
