{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question & Answer System\n",
    "by ES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ES\\Anaconda3\\lib\\site-packages\\gensim-3.4.0-py3.6-win-amd64.egg\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import sys\n",
    "import json \n",
    "import re\n",
    "import gensim\n",
    "import string\n",
    "import math\n",
    "import nltk\n",
    "import random\n",
    "import csv\n",
    "import io\n",
    "import spacy\n",
    "import scipy\n",
    "import heapq\n",
    "from gensim.summarization import bm25\n",
    "from collections import Counter\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open and load files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training.json','r') as f:\n",
    "    training = json.load(f)\n",
    "\n",
    "with open('documents.json','r') as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "with open('devel.json','r') as f:\n",
    "    develop = json.load(f)\n",
    "    \n",
    "with open('testing.json','r') as f:\n",
    "    testing = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT SIMILARITY\n",
    "Use bm25 to calculate the similarity between question and each paragraph in the target document. Get the paragraph with highest score.\n",
    "For test set, return the question and paragraph.\n",
    "For training/develop set, return the index of paragraph, index of right paragraph, and right answer as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting documents dict: 441/441\r"
     ]
    }
   ],
   "source": [
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "def wordPreprocess(word_list):\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    word_list_preprocessed = []\n",
    "    for word in word_list:\n",
    "        word_processed = lemmatize(word.lower())\n",
    "        if word_processed not in stopWords and word_processed not in string.punctuation:\n",
    "            word_list_preprocessed.append(word_processed)\n",
    "    return word_list_preprocessed\n",
    "\n",
    "documents_dict = {}\n",
    "t = str(len(documents))\n",
    "num = 0\n",
    "for doc in documents:\n",
    "    sys.stdout.write(('getting documents dict: '+'{0}/'+t+'\\r').format(num + 1))\n",
    "    sys.stdout.flush()\n",
    "    para_list = []\n",
    "    for para in doc[\"text\"]:\n",
    "        word_list = wordPreprocess(word_tokenize(para))\n",
    "        para_list.append(word_list)\n",
    "    documents_dict[doc[\"docid\"]]=para_list\n",
    "    num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting develop data: 3097/3097\r"
     ]
    }
   ],
   "source": [
    "develop_data = []\n",
    "t = str(len(develop))\n",
    "num = 0\n",
    "for question in develop:\n",
    "    sys.stdout.write(('getting develop data: '+'{0}/'+t+'\\r').format(num + 1))\n",
    "    sys.stdout.flush()\n",
    "    bm25Model = bm25.BM25(documents_dict[question[\"docid\"]])\n",
    "    average_idf = sum(map(lambda k: float(bm25Model.idf[k]), bm25Model.idf.keys())) / len(bm25Model.idf.keys())\n",
    "    scores = bm25Model.get_scores(wordPreprocess(word_tokenize(question[\"question\"])),average_idf)\n",
    "    idx_list = heapq.nlargest(2, enumerate(scores), key=lambda x:x[1])\n",
    "    idx, vals = zip(*idx_list)\n",
    "    develop_data.append((question[\"question\"],[documents[question[\"docid\"]][\"text\"][idx[0]],documents[question[\"docid\"]][\"text\"][idx[0]]],idx,question[\"answer_paragraph\"],question[\"text\"]))\n",
    "    num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting develop data: 3618/3618\r"
     ]
    }
   ],
   "source": [
    "test_data = []\n",
    "t = str(len(testing))\n",
    "num = 0\n",
    "for question in testing:\n",
    "    sys.stdout.write(('getting develop data: '+'{0}/'+t+'\\r').format(num + 1))\n",
    "    sys.stdout.flush()\n",
    "    bm25Model = bm25.BM25(documents_dict[question[\"docid\"]])\n",
    "    average_idf = sum(map(lambda k: float(bm25Model.idf[k]), bm25Model.idf.keys())) / len(bm25Model.idf.keys())\n",
    "    scores = bm25Model.get_scores(word_tokenize(question[\"question\"]),average_idf)\n",
    "    idx_list = heapq.nlargest(2, enumerate(scores), key=lambda x:x[1])\n",
    "    idx, vals = zip(*idx_list)\n",
    "    test_data.append((question[\"question\"],[documents[question[\"docid\"]][\"text\"][idx[0]],documents[question[\"docid\"]][\"text\"][idx[0]]],idx))\n",
    "    num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate function:\n",
    "print the acurate of similarity part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8737487891507911\n"
     ]
    }
   ],
   "source": [
    "def tfidfAcurate(data_list):\n",
    "    data_count = 0\n",
    "    correct_count = 0\n",
    "    for data in data_list:\n",
    "        data_count+=1\n",
    "        if data[2][0]==data[3] or data[2][1]==data[3]:\n",
    "            correct_count +=1\n",
    "    return float(correct_count)/data_count\n",
    "\n",
    "print(tfidfAcurate(develop_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def getEntity(sentence):\n",
    "    ## spacy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(sentence)\n",
    "    return doc.ents\n",
    "\n",
    "def getTextByLabel(entitys,label_list):\n",
    "    answer_list = []\n",
    "    for lab in label_list:\n",
    "        for entity in entitys:\n",
    "            for ent in entity:\n",
    "                label = ent.label_\n",
    "                if label == lab:\n",
    "                    answer_list.append(ent.text)\n",
    "    return answer_list\n",
    "\n",
    "def getAnswer(data_list):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    t = str(len(data_list))\n",
    "    now = 0\n",
    "    answer=[]\n",
    "#     print(type(answer))\n",
    "    id_num = 0\n",
    "    count = [0,0,[0,0,0,0,0,0,0,0],[0,0,0,0],0,[0,0,0,0],0]\n",
    "    useList = [0,0]\n",
    "    for data in data_list:\n",
    "        sys.stdout.write(('{0}/'+t+'\\r').format(id_num + 1))\n",
    "        sys.stdout.flush()\n",
    "        paragraph_entity = [getEntity(data[1][0]),getEntity(data[1][1])]\n",
    "        answer_list = list()\n",
    "        \n",
    "        if bool(re.search('who',data[0],re.IGNORECASE) or re.search('name',data[0],re.IGNORECASE)):\n",
    "            count[0]+=1\n",
    "            answer_list = getTextByLabel(paragraph_entity,[\"PERSON\",\"NORP\",\"ORG\"])\n",
    "        elif bool(re.search('where',data[0],re.IGNORECASE) or re.search('place',data[0],re.IGNORECASE)):\n",
    "            count[1]+=1\n",
    "            answer_list = getTextByLabel(paragraph_entity,[\"GPE\",\"LOC\",\"FACILITY\",\"ORG\"])\n",
    "        elif bool(re.search('what',data[0],re.IGNORECASE)):\n",
    "            if bool(re.search('what time',data[0],re.IGNORECASE)):\n",
    "                count[2][0]+=1\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"DATE\",\"TIME\"])\n",
    "            elif bool(re.search('what year',data[0],re.IGNORECASE) or re.search('what day',data[0],re.IGNORECASE) or re.search('what date',data[0],re.IGNORECASE)):\n",
    "                count[2][1]+=1\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"DATE\"])\n",
    "            elif bool(re.search('what city',data[0],re.IGNORECASE) or re.search('what country',data[0],re.IGNORECASE)):\n",
    "                count[2][2]+=1\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"GPE\"])\n",
    "            elif bool(re.search('what value',data[0],re.IGNORECASE)):\n",
    "                count[2][3]+=1\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"QUANTITY\",\"MONEY\",\"CARDINAL\",\"PERCENT\"])\n",
    "            elif bool(re.search('what percentage',data[0],re.IGNORECASE)):\n",
    "                count[2][4]+=1\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"PERCENT\"])\n",
    "            elif bool(re.search('event',data[0],re.IGNORECASE)):\n",
    "                count[2][5]+=1\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"EVENT\"])\n",
    "            elif bool(re.search('language',data[0],re.IGNORECASE)):\n",
    "                count[2][6]+=1\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"LANGUAGE\"])\n",
    "            else:\n",
    "                count[2][7]+=1\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"PRODUCT\",\"WORK_OF_ART\",\"EVENT\",\"FACILITY\",\"ORG\"])\n",
    "        elif bool(re.search('how',data[0],re.IGNORECASE)):\n",
    "            if bool(re.search('how much',data[0],re.IGNORECASE)):\n",
    "                count[3][0]+=1\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"MONEY\"])\n",
    "            if bool(re.search('how many',data[0],re.IGNORECASE)):\n",
    "                count[3][1]+=1\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"QUANTITY\",\"MONEY\",\"CARDINAL\",\"PERCENT\"])\n",
    "            if bool(re.search('how does',data[0],re.IGNORECASE) or re.search('how was',data[0],re.IGNORECASE)):\n",
    "                count[3][2]+=1\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"LAW\",\"EVENT\"])\n",
    "            else:\n",
    "                count[3][3]+=1\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"QUANTITY\",\"MONEY\",\"CARDINAL\"])\n",
    "        elif bool(re.search('when',data[0],re.IGNORECASE)):\n",
    "            count[4]+=1\n",
    "            answer_list = getTextByLabel(paragraph_entity,[\"DATE\",\"TIME\"])\n",
    "        elif bool(re.search('which',data[0],re.IGNORECASE)):\n",
    "            if bool(re.search('which person',data[0],re.IGNORECASE)):\n",
    "                count[5][0]+=1\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"PERSON\"])\n",
    "            elif bool(re.search('which time',data[0],re.IGNORECASE)):\n",
    "                count[5][1]+=1\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"DATE\",\"TIME\"])\n",
    "            elif bool(re.search('which country',data[0],re.IGNORECASE)):\n",
    "                count[5][2]+=1\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"NORP\",\"ORG\"])\n",
    "            else:\n",
    "                count[5][3]+=1\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"FACILITY\",\"PRODUCT\",\"WORK_OF_ART\",\"EVENT\",\"ORG\"])\n",
    "        else:\n",
    "            count[6]+=1\n",
    "        for ans in answer_list:\n",
    "            if ans in data[0]:\n",
    "                answer_list.remove(ans)\n",
    "                \n",
    "        if len(answer_list)==0:\n",
    "            useList[1]+=1\n",
    "            \n",
    "            try:\n",
    "                if not len(paragraph_entity[0])==0:\n",
    "                    for ent in paragraph_entity[0]:\n",
    "                        if ent.text in data[0]:\n",
    "                            paragraph_entity[0].remove(ent)\n",
    "                    index = random.randint(0,len(paragraph_entity[0])-1) \n",
    "                    answer.append (([str(id_num)+','+paragraph_entity[0][index].text.replace(',','')],data[2]))\n",
    "                else:\n",
    "                    for ent in paragraph_entity[1]:\n",
    "                        if ent.text in data[0]:\n",
    "                            paragraph_entity[1].remove(ent)\n",
    "                    index = random.randint(0,len(paragraph_entity[1])-1) \n",
    "                    answer.append (([str(id_num)+','+paragraph_entity[1][index].text.replace(',','')],data[2]))\n",
    "            except:\n",
    "                doc = nlp(data[1][0])\n",
    "                index = random.randint(0,len(doc)-1)\n",
    "                answer.append(([str(id_num)+','+doc[index].text.replace(',','')],data[2]))\n",
    "        else:\n",
    "            try:\n",
    "                if answer_list [0] not in data[0]:\n",
    "#                     print(answer_list[0])\n",
    "                    answer.append(([str(id_num)+','+answer_list[0].replace(',','')],data[2]))\n",
    "                elif answer_list [1] not in data[0]:\n",
    "                    answer.append(([str(id_num)+','+answer_list[1].replace(',','')],data[2]))\n",
    "                elif answer_list [2] not in data[0]:\n",
    "                    answer.append(([str(id_num)+','+answer_list[2].replace(',','')],data[2]))\n",
    "                else:\n",
    "                    answer.append(([str(id_num)+','+answer_list[3].replace(',','')],data[2]))\n",
    "            except:\n",
    "                doc = nlp(data[1][0])\n",
    "                index = random.randint(0,len(doc)-1)\n",
    "                answer.append(([str(id_num)+','+doc[index].text.replace(',','')],data[2]))\n",
    "            useList[0]+=1\n",
    "        id_num +=1\n",
    "    print(\"count \",count)\n",
    "    print(\"use list \",useList)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/3097\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-9008c5f7a169>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# answer = getAnswer(test_data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetAnswer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevelop_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-32-16a31c632429>\u001b[0m in \u001b[0;36mgetAnswer\u001b[1;34m(data_list)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{0}/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid_num\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mparagraph_entity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgetEntity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgetEntity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0manswer_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-16a31c632429>\u001b[0m in \u001b[0;36mgetEntity\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetEntity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m## spacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_sm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# in data dir / shortcut\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_link\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_package\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# installed as package\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_package\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_link\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE051\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\data\\en_core_web_sm\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(**overrides)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_init_py\u001b[1;34m(init_file, **overrides)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE052\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath2str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[1;34m(model_path, meta, **overrides)\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[0mmeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_model_meta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_lang_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lang'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m     \u001b[0mpipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pipeline'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[0mdisable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moverrides\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'disable'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, vocab, make_doc, max_length, meta, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmake_doc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m             \u001b[0mfactory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDefaults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_tokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m             \u001b[0mmake_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfactory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mmeta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_doc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mcreate_tokenizer\u001b[1;34m(cls, nlp)\u001b[0m\n\u001b[0;32m     69\u001b[0m                          \u001b[0msuffix_search\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msuffix_search\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                          \u001b[0minfix_finditer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minfix_finditer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m                          token_match=token_match)\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mpipe_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'tagger'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'parser'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ner'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mtokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mtokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.add_special_case\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mvocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab.make_fused_token\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mvocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab.get_by_orth\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mvocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab._new_lexeme\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\lang\\lex_attrs.py\u001b[0m in \u001b[0;36mlike_email\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlike_email\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_like_email\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# answer = getAnswer(test_data)\n",
    "answer = getAnswer(develop_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate function:\n",
    "print the acurate of answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def campare(question,answer):\n",
    "    if question[4].lower()==str(answer[0]).strip(\"'\").split(',',1)[1][:-2].lower():\n",
    "        return True\n",
    "    else:\n",
    "        print(question[0],\"answer: \"+question[4],\"your answer: \"+str(answer[0]).strip(\"'\").split(',',1)[1][:-2])\n",
    "        return False\n",
    "\n",
    "def answerAcurate(data_list,answer):\n",
    "    data_count = 0\n",
    "    correct_count = 0\n",
    "    results = map(campare,data_list,answer)\n",
    "    for result in results:\n",
    "        data_count+=1\n",
    "        if result:\n",
    "            correct_count +=1\n",
    "    return float(correct_count)/data_count\n",
    "\n",
    "print(answerAcurate(develop_data,answer))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Write into csv file\n",
    "def writeFile(answer):\n",
    "#     sys.stdout = io.TextIOWrapper(sys.stdout.buffer,encoding='utf8')\n",
    "    out = open('answer.csv','a',newline='')\n",
    "    csv_write =csv.writer(out,dialect='excel')\n",
    "    csv_write.writerow([\"id,answer\"])\n",
    "    answerID = 0\n",
    "    for line in answer:\n",
    "        try:\n",
    "            csv_write.writerow(line[0])\n",
    "        except:\n",
    "            print(\"line \"+str(answerID)+\" cannot writeï¼š \"+str(line[0]))\n",
    "        answerID+=1\n",
    "    out.close()\n",
    "    \n",
    "writeFile(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
