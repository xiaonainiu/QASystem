{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question & Answer System\n",
    "by ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import urllib\n",
    "import sys\n",
    "import os\n",
    "import zipfile\n",
    "import tarfile\n",
    "import json \n",
    "import hashlib\n",
    "import re\n",
    "import itertools\n",
    "import gensim\n",
    "import string\n",
    "import math\n",
    "import nltk\n",
    "import random\n",
    "import csv\n",
    "import io\n",
    "import spacy\n",
    "import scipy\n",
    "from gensim.summarization import bm25\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from importlib import reload\n",
    "# from spacy.en import English\n",
    "from collections import Counter\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting documents dict: 441/441\r"
     ]
    }
   ],
   "source": [
    "with open('training.json','r') as f:\n",
    "    training = json.load(f)\n",
    "\n",
    "with open('documents.json','r') as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "with open('devel.json','r') as f:\n",
    "    develop = json.load(f)\n",
    "    \n",
    "with open('testing.json','r') as f:\n",
    "    testing = json.load(f)\n",
    "\n",
    "\n",
    "documents_dict = {}\n",
    "t = str(len(documents))\n",
    "num = 0\n",
    "for doc in documents:\n",
    "    sys.stdout.write(('getting documents dict: '+'{0}/'+t+'\\r').format(num + 1))\n",
    "    sys.stdout.flush()\n",
    "    para_list = []\n",
    "    for para in doc[\"text\"]:\n",
    "        word_list = word_tokenize(para)\n",
    "        para_list.append(word_list)\n",
    "    documents_dict[doc[\"docid\"]]=para_list\n",
    "    num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting develop data: 3097/3097\r"
     ]
    }
   ],
   "source": [
    "develop_data = []\n",
    "t = str(len(develop))\n",
    "num = 0\n",
    "for question in develop:\n",
    "    sys.stdout.write(('getting develop data: '+'{0}/'+t+'\\r').format(num + 1))\n",
    "    sys.stdout.flush()\n",
    "    bm25Model = bm25.BM25(documents_dict[question[\"docid\"]])\n",
    "    average_idf = sum(map(lambda k: float(bm25Model.idf[k]), bm25Model.idf.keys())) / len(bm25Model.idf.keys())\n",
    "    scores = bm25Model.get_scores(word_tokenize(question[\"question\"]),average_idf)\n",
    "    idx = scores.index(max(scores))\n",
    "    \n",
    "    \n",
    "    develop_data.append((question[\"question\"],documents[question[\"docid\"]][\"text\"][idx],idx,question[\"answer_paragraph\"],question[\"text\"]))\n",
    "    num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.word2vec.Word2Vec'>\n"
     ]
    }
   ],
   "source": [
    "sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)\n",
    "print(type(model))\n",
    "# print(model.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7203745560219568\n"
     ]
    }
   ],
   "source": [
    "def tfidfAcurate(data_list):\n",
    "    data_count = 0\n",
    "    correct_count = 0\n",
    "    for data in data_list:\n",
    "#         print(data)\n",
    "        data_count+=1\n",
    "#         print(data[2],data[3])\n",
    "        if data[2]==data[3]:\n",
    "            correct_count +=1\n",
    "    return float(correct_count)/data_count\n",
    "\n",
    "print(tfidfAcurate(develop_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEntity(sentence):\n",
    "    ## spacy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(sentence)\n",
    "    return doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def getTextByLabel(entity,label_list):\n",
    "    answer_list = []\n",
    "    for ent in entity:\n",
    "        label = ent.label_\n",
    "        if label in label_list:\n",
    "            answer_list.append(ent.text)\n",
    "    return answer_list\n",
    "\n",
    "def getAnswer(data_list):\n",
    "    t = str(len(data_list))\n",
    "    now = 0\n",
    "    answer=[]\n",
    "    id_num = 0\n",
    "    count = [0,0,[0,0,0],[0,0,0,0],0,[0,0,0,0],0]\n",
    "    useList = [0,0]\n",
    "    for data in data_list:\n",
    "        sys.stdout.write(('{0}/'+t+'\\r').format(id_num + 1))\n",
    "        sys.stdout.flush()\n",
    "        paragraph_entity = getEntity(data[1])\n",
    "        answer_list = list()\n",
    "        \n",
    "        if bool(re.search('who',data[0],re.IGNORECASE)):\n",
    "            count[0]+=1\n",
    "            answer_list = getTextByLabel(paragraph_entity,[\"PERSON\",\"NORP\",\"ORG\"])\n",
    "        elif bool(re.search('where',data[0],re.IGNORECASE)):\n",
    "            count[1]+=1\n",
    "            answer_list = getTextByLabel(paragraph_entity,[\"FACILITY\",\"GPE\",\"LOC\"])\n",
    "        elif bool(re.search('what',data[0],re.IGNORECASE)):\n",
    "            if bool(re.search('what time',data[0],re.IGNORECASE)):\n",
    "                count[2][0]+=1\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"DATE\",\"TIME\"])\n",
    "            elif bool(re.search('what year',data[0],re.IGNORECASE) or re.search('what day',data[0],re.IGNORECASE) or re.search('what date',data[0],re.IGNORECASE)):\n",
    "                count[2][1]+=1\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"DATE\"])\n",
    "            elif bool(re.search('what city',data[0],re.IGNORECASE or re.search('what country',data[0],re.IGNORECASE)):\n",
    "                count[2][2]+=1\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"GPE\"])\n",
    "            else:\n",
    "                count[2][2]+=1\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"FACILITY\",\"PRODUCT\",\"WORK_OF_ART\",\"EVENT\",\"ORG\"])\n",
    "        elif bool(re.search('how',data[0],re.IGNORECASE)):\n",
    "            if bool(re.search('how much',data[0],re.IGNORECASE)):\n",
    "                count[3][0]+=1\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"MONEY\"])\n",
    "            if bool(re.search('how many',data[0],re.IGNORECASE)):\n",
    "                count[3][1]+=1\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"MONEY\",\"QUANTITY\",\"CARDINAL\"])\n",
    "            if bool(re.search('how does',data[0],re.IGNORECASE) or re.search('how was',data[0],re.IGNORECASE)):\n",
    "                count[3][2]+=1\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"LAW\",\"EVENT\",\"PRODUCT\",\"PERSON\"])\n",
    "            else:\n",
    "                count[3][3]+=1\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"MONEY\",\"QUANTITY\",\"CARDINAL\"])\n",
    "        elif bool(re.search('when',data[0],re.IGNORECASE)):\n",
    "            count[4]+=1\n",
    "            answer_list = getTextByLabel(paragraph_entity,[\"DATE\",\"TIME\"])\n",
    "        elif bool(re.search('which',data[0],re.IGNORECASE)):\n",
    "            if bool(re.search('which person',data[0],re.IGNORECASE)):\n",
    "                count[5][0]+=1\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"PERSON\"])\n",
    "            elif bool(re.search('which time',data[0],re.IGNORECASE)):\n",
    "                count[5][1]+=1\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"DATE\",\"TIME\"])\n",
    "            elif bool(re.search('which country',data[0],re.IGNORECASE)):\n",
    "                count[5][2]+=1\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"NORP\",\"ORG\"])\n",
    "            else:\n",
    "                count[5][3]+=1\n",
    "                answer_list = getTextByLabel(paragraph_entity,[\"FACILITY\",\"PRODUCT\",\"WORK_OF_ART\",\"EVENT\",\"ORG\"])\n",
    "        else:\n",
    "            count[6]+=1\n",
    "        if len(answer_list)==0:\n",
    "            useList[1]+=1\n",
    "            try:\n",
    "                index = random.randint(0,len(paragraph_entity)-1) \n",
    "                answer.append (([str(id_num)+','+paragraph_entity[index].text.replace(\",\", \"\")],data[2]))\n",
    "            except:\n",
    "                index = random.randint(0,len(data[1])-1)\n",
    "                answer.append(([str(id_num)+','+data[1][index].replace(\",\", \"\")],data[2]))\n",
    "        else:\n",
    "            useList[0]+=1\n",
    "            index = random.randint(0,len(answer_list)-1)\n",
    "            answer.append (([str(id_num)+','+answer_list[index].replace(\",\", \"\")],data[2]))\n",
    "        id_num +=1\n",
    "    print(\"count \",count)\n",
    "    print(\"use list \",useList)\n",
    "    return answer\n",
    "\n",
    "    \n",
    "\n",
    "# for line in answer:\n",
    "#     print(line)\n",
    "# count  [2198, 279, 422, 156, 563]\n",
    "# use list  [2757, 861]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2633/3097\r"
     ]
    }
   ],
   "source": [
    "# answer = getAnswer(test_data)\n",
    "answer = getAnswer(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def campare(question,answer):\n",
    "    if question[4]==str(answer[0]).strip(\"'\").split(',')[1][:-2]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def answerAcurate(data_list,answer):\n",
    "    data_count = 0\n",
    "    correct_count = 0\n",
    "    results = map(campare,data_list,answer)\n",
    "    for result in results:\n",
    "        data_count+=1\n",
    "        if result:\n",
    "            correct_count +=1\n",
    "    return float(correct_count)/data_count\n",
    "\n",
    "print(answerAcurate(develop_data,answer))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line 81 cannot write： ['81,Galschiøt']\n",
      "line 136 cannot write： ['136,Galschiøt']\n",
      "line 220 cannot write： ['220,Galschiøt']\n",
      "line 222 cannot write： ['222,Galschiøt']\n",
      "line 261 cannot write： ['261,Điếu Cày']\n",
      "line 558 cannot write： ['558,£100m']\n",
      "line 806 cannot write： ['806,Aškuzai/Iškuzai']\n",
      "line 835 cannot write： ['835,Ποσειδώνας']\n",
      "line 1075 cannot write： ['1075,Calçada Portuguesa']\n",
      "line 1095 cannot write： ['1095,Calçada Portuguesa']\n",
      "line 1096 cannot write： ['1096,the River Mureş']\n",
      "line 1163 cannot write： ['1163,Calçada Portuguesa']\n",
      "line 1166 cannot write： ['1166,Calçada Portuguesa']\n",
      "line 1353 cannot write： ['1353,Åland Islands']\n",
      "line 1928 cannot write： ['1928,Mötley Crüe']\n",
      "line 2188 cannot write： [\"2188,François Bernier's\"]\n",
      "line 2259 cannot write： ['2259,Štrkalj']\n",
      "line 2328 cannot write： [\"2328,François Bernier's\"]\n",
      "line 2388 cannot write： ['2388,the Norte Chico Civilization Formative Mesoamerica and Ancient Hawaiʻi']\n",
      "line 2392 cannot write： ['2392,the Norte Chico Civilization Formative Mesoamerica and Ancient Hawaiʻi']\n",
      "line 2405 cannot write： ['2405,the Norte Chico Civilization Formative Mesoamerica and Ancient Hawaiʻi']\n",
      "line 2410 cannot write： ['2410,Göbekli Tepe']\n",
      "line 2671 cannot write： ['2671,between £4.99 and £11.99']\n",
      "line 2740 cannot write： ['2740,El Niño']\n",
      "line 3465 cannot write： ['3465,אַ שפּראַך איז אַ דיאַלעקט מיט אַן אַרמײ און פֿלאָט\"\\u200e:']\n"
     ]
    }
   ],
   "source": [
    "##Write into csv file\n",
    "def writeFile(answer):\n",
    "#     sys.stdout = io.TextIOWrapper(sys.stdout.buffer,encoding='utf8')\n",
    "    out = open('answer.csv','a',newline='')\n",
    "    csv_write =csv.writer(out,dialect='excel')\n",
    "    csv_write.writerow([\"id,answer\"])\n",
    "    answerID = 0\n",
    "    for line in answer:\n",
    "        try:\n",
    "            csv_write.writerow(line[0])\n",
    "        except:\n",
    "            print(\"line \"+str(answerID)+\" cannot write： \"+str(line[0]))\n",
    "        answerID+=1\n",
    "    out.close()\n",
    "    \n",
    "writeFile(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
