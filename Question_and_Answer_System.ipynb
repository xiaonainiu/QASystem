{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question & Answer System\n",
    "by ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-dc727aabc2bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import urllib\n",
    "import sys\n",
    "import os\n",
    "import zipfile\n",
    "import tarfile\n",
    "import json \n",
    "import hashlib\n",
    "import re\n",
    "import itertools\n",
    "import gensim\n",
    "import string\n",
    "import math\n",
    "import nltk\n",
    "import random\n",
    "import csv\n",
    "import io\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.tree import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.word2vec.Word2Vec'>\n"
     ]
    }
   ],
   "source": [
    "sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)\n",
    "print(type(model))\n",
    "# print(model.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def toWordList(str):\n",
    "    word_list = []\n",
    "    words = str.split()\n",
    "    for word in words:\n",
    "        wordd = word.strip(',.?\\'\\\")(').lower()\n",
    "        word_list.append(wordd)\n",
    "    return word_list\n",
    "\n",
    "documents_list = {}\n",
    "documents_count = {}\n",
    "word_set_dic ={}\n",
    "with open('documents.json','r') as f:\n",
    "    documents = json.load(f)\n",
    "    for dic in documents:\n",
    "        word_set = set()\n",
    "        para_list = []\n",
    "        document_list = []\n",
    "        for sentence in dic[\"text\"]:\n",
    "            word_count=Counter()\n",
    "            word_list = toWordList(sentence)\n",
    "            for word in word_list:\n",
    "                word_count[word]+=1\n",
    "                word_set.add(word)\n",
    "            para_list.append(word_list)\n",
    "            document_list.append(word_count)\n",
    "        documents_list[dic[\"docid\"]] = para_list\n",
    "        documents_count[dic[\"docid\"]] = document_list\n",
    "        word_set_dic[dic[\"docid\"]]=set(word_set)\n",
    "\n",
    "# for key in documents_count:\n",
    "#     print (documents_count[key])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(documents_count[410][0]['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTF(docid,word_count):\n",
    "    tfVector = []\n",
    "    for word in word_set_dic[docid]:\n",
    "        tfVector.append(word_count[word])\n",
    "    return tfVector\n",
    "\n",
    "def getIDF(question,docid):\n",
    "    idfVector = []\n",
    "    for word in word_set_dic[docid]:\n",
    "        if word in question:\n",
    "            count = 0\n",
    "            for para in documents_list[docid]:\n",
    "                if word in para:\n",
    "                    count +=1\n",
    "            if not count == 0:\n",
    "                idfVector.append( float( len(documents_list[docid]) )/count )\n",
    "            else:\n",
    "                idfVector.append(0)\n",
    "        else:\n",
    "            idfVector.append(0)\n",
    "    return idfVector\n",
    "\n",
    "def getParagraph(question,docid):\n",
    "    tfidf_list=[]\n",
    "    for paraIndex in documents_count[docid]:\n",
    "        tfVector = getTF(docid,paraIndex)\n",
    "        idfVector = getIDF(question,docid)\n",
    "        zip_list = list(zip(tfVector,idfVector))\n",
    "        f = lambda x,y:x*y\n",
    "        prod = sum(list(map(f,tfVector,idfVector)))\n",
    "        cosine = math.sqrt(np.sum(np.square(tfVector)))*math.sqrt(np.sum(np.square(idfVector)))\n",
    "        if not cosine == 0:\n",
    "            tfidf_list.append(float(prod)/cosine)\n",
    "        else:\n",
    "            tfidf_list.append(0)\n",
    "#     print (docid,np.argmax(tfidf_list),tfidf_list)\n",
    "#     print(documents[docid][\"text\"][np.argmax(tfidf_list)])\n",
    "    return documents[docid][\"text\"][np.argmax(tfidf_list)],np.argmax(tfidf_list)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('training.json','r') as f:\n",
    "    training = json.load(f)\n",
    "\n",
    "with open('devel.json','r') as f:\n",
    "    develop = json.load(f)\n",
    "    \n",
    "    develop_data = []\n",
    "    for question in develop:\n",
    "        question_word_list = toWordList(question[\"question\"])\n",
    "        paragraph,index = getParagraph(question_word_list,question[\"docid\"])\n",
    "        develop_data.append((question[\"question\"],paragraph,index,question[\"answer_paragraph\"],question[\"text\"]))\n",
    "    \n",
    "# with open('testing.json','r') as f:\n",
    "#     testing = json.load(f)\n",
    "#     answerParagraph = []\n",
    "#     test_data = []\n",
    "#     for question in testing:\n",
    "#         question_word_list = toWordList(question[\"question\"])\n",
    "#         paragraph,index = getParagraph(question_word_list,question[\"docid\"])\n",
    "#         test_data.append((question[\"question\"],paragraph,index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6951888924765902\n"
     ]
    }
   ],
   "source": [
    "def tfidfAcurate(data_list):\n",
    "    data_count = 0\n",
    "    correct_count = 0\n",
    "    for data in data_list:\n",
    "        data_count+=1\n",
    "#         print(data[2],data[3])\n",
    "        if data[2]==data[3]:\n",
    "            correct_count +=1\n",
    "    return float(correct_count)/data_count\n",
    "\n",
    "print(tfidfAcurate(develop_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addEntity(sentence):\n",
    "    tagged = pos_tag(word_tokenize(sentence))\n",
    "    entity = nltk.chunk.ne_chunk(tagged)\n",
    "    return entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def getEntity(data_list):\n",
    "    for data in data_list:\n",
    "        question_entity = addEntity(data[0])\n",
    "        paragraph_entity = addEntity(data[1])\n",
    "\n",
    "def getAnswer(data_list):\n",
    "    answer=[]\n",
    "    id_num = 0\n",
    "    count = [0,0,0,0,0]\n",
    "    useList = [0,0]\n",
    "    for data in data_list:\n",
    "        question_entity = addEntity(data[0])\n",
    "        paragraph_entity = addEntity(data[1])\n",
    "        answer_list = []\n",
    "        \n",
    "        if bool(re.search('what',data[0],re.IGNORECASE)):\n",
    "            count[0]+=1\n",
    "            for tree in paragraph_entity:\n",
    "                try:\n",
    "                    if tree[1] == \"NN\":\n",
    "                        answer_list.append(tree[0])\n",
    "                except:\n",
    "                    pass\n",
    "        elif bool(re.search('when',data[0],re.IGNORECASE)):\n",
    "            count[1]+=1\n",
    "            for tree in paragraph_entity:\n",
    "                try:\n",
    "                    if tree.label()==\"TIME\":\n",
    "                        name = \"\"\n",
    "                        for sub_tree in tree:\n",
    "                            name.append(sub_tree[0]+\" \")\n",
    "                        answer_list.append(name[:-1])\n",
    "                    if tree.label()==\"DATE\":\n",
    "                        name = \"\"\n",
    "                        for sub_tree in tree:\n",
    "                            name.append(sub_tree[0]+\" \")\n",
    "                        answer_list.append(name[:-1])\n",
    "                    if tree[1] == \"CD\":\n",
    "                        answer_list.append(tree[0])\n",
    "                except:\n",
    "                    pass\n",
    "        elif bool(re.search('who',data[0],re.IGNORECASE)):\n",
    "            count[2]+=1\n",
    "            for tree in paragraph_entity:\n",
    "                try:\n",
    "                    if tree.label()==\"PERSON\":\n",
    "                        name = \"\"\n",
    "                        for sub_tree in tree:\n",
    "                            name.append(sub_tree[0]+\" \")\n",
    "                        answer_list.append(name[:-1])\n",
    "                except:\n",
    "                    pass\n",
    "        elif bool(re.search('where',data[0],re.IGNORECASE)):\n",
    "            count[3]+=1\n",
    "            for tree in paragraph_entity:\n",
    "                try:\n",
    "                    if tree.label()==\"LOCATION\":\n",
    "                        name = \"\"\n",
    "                        for sub_tree in tree:\n",
    "                            name.append(sub_tree[0]+\" \")\n",
    "                        answer_list.append(name[:-1])\n",
    "                    if tree.label()==\"gpe\":\n",
    "                        name = \"\"\n",
    "                        for sub_tree in tree:\n",
    "                            name.append(sub_tree[0]+\" \")\n",
    "                        answer_list.append(name[:-1])\n",
    "                except:\n",
    "                    pass\n",
    "        else:\n",
    "            count[4]+=1\n",
    "            for tree in paragraph_entity:\n",
    "                try:\n",
    "                    if tree[1] == \"NN\":\n",
    "                        answer_list.append(tree[0])\n",
    "                except:\n",
    "                    pass\n",
    "        if len(answer_list)==0:\n",
    "            useList[1]+=1\n",
    "            index = random.randint(0,len(paragraph_entity)-1)\n",
    "            try:\n",
    "                answer.append (([str(id_num)+','+paragraph_entity[index][0].replace(\",\", \"\")],data[2]))\n",
    "#                 print([str(id_num)+','+paragraph_entity[index][0].replace(\",\", \"\")],data[2])\n",
    "            except:\n",
    "                answer.append(([str(id_num)+','+paragraph_entity[index][0][0].replace(\",\", \"\")],data[2]))\n",
    "#                 print([str(id_num)+','+paragraph_entity[index][0][0].replace(\",\", \"\")],data[2])\n",
    "        else:\n",
    "            useList[0]+=1\n",
    "            index = random.randint(0,len(answer_list)-1)\n",
    "#             print([str(id_num)+','+answer_list[index]],data[2])\n",
    "            answer.append (([str(id_num)+','+answer_list[index].replace(\",\", \"\")],data[2]))\n",
    "        id_num +=1\n",
    "    print(\"count \",count)\n",
    "    print(\"use list \",useList)\n",
    "    return answer\n",
    "\n",
    "    \n",
    "\n",
    "# for line in answer:\n",
    "#     print(line)\n",
    "# count  [2198, 279, 422, 156, 563]\n",
    "# use list  [2757, 861]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count  [1855, 163, 365, 107, 607]\n",
      "use list  [2462, 635]\n"
     ]
    }
   ],
   "source": [
    "# answer = getAnswer(test_data)\n",
    "answer = getAnswer(develop_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004197610590894414\n"
     ]
    }
   ],
   "source": [
    "def campare(question,answer):\n",
    "#     print(question,answer)\n",
    "#     print(str(answer[0]).strip(\"'\").split(',')[1][:-2])\n",
    "#     print(question[4])\n",
    "    \n",
    "    if question[4]==str(answer[0]).strip(\"'\").split(',')[1][:-2]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def answerAcurate(data_list,answer):\n",
    "    data_count = 0\n",
    "    correct_count = 0\n",
    "    results = map(campare,data_list,answer)\n",
    "    for result in results:\n",
    "        data_count+=1\n",
    "        if result:\n",
    "            correct_count +=1\n",
    "    return float(correct_count)/data_count\n",
    "\n",
    "print(answerAcurate(develop_data,answer))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line 730 cannot write\n",
      "line 731 cannot write\n",
      "line 951 cannot write\n",
      "line 2149 cannot write\n",
      "line 2562 cannot write\n"
     ]
    }
   ],
   "source": [
    "##Write into csv file\n",
    "def writeFile(answer):\n",
    "    # sys.stdout = io.TextIOWrapper(sys.stdout.buffer,encoding='utf8')\n",
    "    out = open('answer.csv','a',newline='')\n",
    "    csv_write =csv.writer(out,dialect='excel')\n",
    "    csv_write.writerow([\"id,answer\"])\n",
    "    answerID = 0\n",
    "    for line in answer:\n",
    "#         print(line[0])\n",
    "        try:\n",
    "            csv_write.writerow(line[0])\n",
    "        except:\n",
    "            print(\"line \"+str(answerID)+\" cannot write\")\n",
    "        answerID+=1\n",
    "    out.close()\n",
    "    \n",
    "writeFile(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
